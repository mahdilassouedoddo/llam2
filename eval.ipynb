{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    \"C:\\\\Users\\\\MLASSOUED\\Documents\\\\llam2\\\\data\"\n",
    "\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SentenceSplitter(chunk_size=512)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"node_{idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 31 key-value pairs and 245 tensors from C:\\projects\\Phi-3-medium-128k-instruct-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 131072\n",
      "llama_model_loader: - kv   3:  phi3.rope.scaling.original_context_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                      phi3.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   5:                   phi3.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                           phi3.block_count u32              = 40\n",
      "llama_model_loader: - kv   7:                  phi3.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:               phi3.attention.head_count_kv u32              = 10\n",
      "llama_model_loader: - kv   9:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                  phi3.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  11:                        phi3.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  13:              phi3.rope.scaling.attn_factor f32              = 1.190238\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% for message in messages %}{% if (m...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  27:                      quantize.imatrix.file str              = /models/Phi-3-medium-128k-instruct-GG...\n",
      "llama_model_loader: - kv  28:                   quantize.imatrix.dataset str              = /training_data/calibration_data.txt\n",
      "llama_model_loader: - kv  29:             quantize.imatrix.entries_count i32              = 160\n",
      "llama_model_loader: - kv  30:              quantize.imatrix.chunks_count i32              = 234\n",
      "llama_model_loader: - type  f32:   83 tensors\n",
      "llama_model_loader: - type q5_K:  101 tensors\n",
      "llama_model_loader: - type q6_K:   61 tensors\n",
      "llm_load_vocab: special tokens cache size = 67\n",
      "llm_load_vocab: token to piece cache size = 0.1690 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi3\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 10\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 131072\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1280\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1280\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 14B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 13.96 B\n",
      "llm_load_print_meta: model size       = 9.38 GiB (5.77 BPW) \n",
      "llm_load_print_meta: general.name     = Phi3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.13 MiB\n",
      "llm_load_tensors:        CPU buffer size =  9606.79 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3904\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   762.50 MiB\n",
      "llama_new_context_with_model: KV self size  =  762.50 MiB, K (f16):  381.25 MiB, V (f16):  381.25 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   357.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1606\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.model': 'llama', 'phi3.feed_forward_length': '17920', 'phi3.rope.scaling.original_context_length': '4096', 'general.name': 'Phi3', 'general.architecture': 'phi3', 'phi3.attention.head_count_kv': '10', 'phi3.context_length': '131072', 'phi3.embedding_length': '5120', 'phi3.block_count': '40', 'phi3.attention.head_count': '40', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.rope.dimension_count': '128', 'tokenizer.chat_template': \"{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'phi3.rope.freq_base': '10000.000000', 'tokenizer.ggml.eos_token_id': '32000', 'general.file_type': '17', 'phi3.rope.scaling.attn_factor': '1.190238', 'tokenizer.ggml.pre': 'default', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.chunks_count': '234', 'quantize.imatrix.file': '/models/Phi-3-medium-128k-instruct-GGUF/Phi-3-medium-128k-instruct.imatrix', 'quantize.imatrix.dataset': '/training_data/calibration_data.txt', 'quantize.imatrix.entries_count': '160'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, load_index_from_storage, StorageContext\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    #model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=\"C:\\projects\\Phi-3-medium-128k-instruct-Q5_K_M.gguf\",\n",
    "    temperature=0,\n",
    "    max_new_tokens=500,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 0},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "from huggingface_hub import configure_http_backend\n",
    "import os\n",
    "import urllib3\n",
    "import requests\n",
    " \n",
    "def backend_factory() -> requests.Session:\n",
    "    session = requests.Session()\n",
    "    session.verify = False\n",
    "    return session\n",
    " \n",
    "configure_http_backend(backend_factory=backend_factory)\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "index.storage_context.persist(persist_dir='./storage')\n",
    "index.set_index_id(\"vector_index\")\n",
    "index.storage_context.persist(\"./storage\")\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "index = load_index_from_storage(storage_context, index_id=\"vector_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 6781c747-3e17-4fb7-b693-3e59f2bc65bb<br>**Similarity:** 0.7290516096275045<br>**Text:** Dihydrogène (H2) 120 – 142 (liquéfié) 8,5 – 10,1 (Production et liquéfaction de l'hydrogène) 0,0\n",
       "Carburants d'origine fossile\n",
       "Charbon 29,3 – 33,5 39,85 - 74,43(En ne comptant pas : CO, NOx, sulfates et particules) : ~3,59\n",
       "Pétrole 41,868 28 – 31,4 (En ne comptant pas : CO, NOx, sulfates et particules) : ~3,4\n",
       "Essence 45 – 48,3 32 – 34,8(En ne comptant pas : CO, NOx, sulfates et particules) : ~3,30\n",
       "Gazole (Diesel) 48,1 40,3 (En ne comptant pas : CO, NOx, sulfates et particules) : ~3,4\n",
       "Gaz naturel 38 – 50(liquéfié) 25,5 –\n",
       "28,7(Éthane, propane et butane N/C : CO, NOx et sulfates) : ~3,00\n",
       "Éthane (CH3-CH3) 51,9 (liquéfié) ~24,0 2,93\n",
       "Pouvoir calorifique inférieur (PCI) de composés organiques purs (à29/08/2024 09:45 Carburant — Wikipédia\n",
       "https://fr.wikipedia.org/wiki/Carburant 4/8<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 1cdd38a0-7d04-4e6c-bec7-19b8bd6162a1<br>**Similarity:** 0.7081434882530891<br>**Text:** 9. Dans de nombreuses régions agricoles de l’OCDE, les niveaux de pollution dépassent les normes de qualité de\n",
       "l’eau potable  (http://www .oecd.org/document/36/0,3343,fr_2649_34487_40846244_1_1_1_1,00.html) , sur\n",
       "oecd.org, consulté le 28 décembre 2018\n",
       "Jean-Claude Guibet  et Emmanuelle Faure , Carburants et\n",
       "moteurs : technologies, énergie, environnement , Ophrys, 1997 ,\n",
       "819 p. (ISBN 978-2-7108-0704-9, lire en ligne (https://books.google.fr/books?id=IpUnMfB27i4C&printsec=frontcover))\n",
       "E10 (carburant)\n",
       "E85 (carburant)Distributeurs pétroliers\n",
       "Notes et références\n",
       "Notes\n",
       "Références\n",
       "Annexes\n",
       "Sur les autres projets Wikimedia :\n",
       "carburant , sur le Wiktionnaire\n",
       "Une catégorie  est consacrée à ce\n",
       "sujet : Carburant .Bibliographie\n",
       "Articles connexes29/08/2024 09:45 Carburant — Wikipédia\n",
       "https://fr.wikipedia.org/wiki/Carburant 7/8<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=2)\n",
    "retrieved_nodes = retriever.retrieve(\"les carburants fossiles?\")\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    display_source_node(node, source_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =       8.98 ms /    52 runs   (    0.17 ms per token,  5791.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =  247670.98 ms /   686 tokens (  361.04 ms per token,     2.77 tokens per second)\n",
      "llama_print_timings:        eval time =   72470.40 ms /    51 runs   ( 1420.99 ms per token,     0.70 tokens per second)\n",
      "llama_print_timings:       total time =  322709.35 ms /   737 tokens\n",
      "  5%|▌         | 1/20 [05:22<1:42:13, 322.82s/it]Llama.generate: 71 prefix-match hit, remaining 543 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =       8.33 ms /    61 runs   (    0.14 ms per token,  7322.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =  193768.80 ms /   543 tokens (  356.85 ms per token,     2.80 tokens per second)\n",
      "llama_print_timings:        eval time =   30787.78 ms /    60 runs   (  513.13 ms per token,     1.95 tokens per second)\n",
      "llama_print_timings:       total time =  224866.40 ms /   603 tokens\n",
      " 10%|█         | 2/20 [09:07<1:19:34, 265.25s/it]Llama.generate: 71 prefix-match hit, remaining 599 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =      10.79 ms /    74 runs   (    0.15 ms per token,  6855.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =  193114.17 ms /   599 tokens (  322.39 ms per token,     3.10 tokens per second)\n",
      "llama_print_timings:        eval time =   37249.30 ms /    73 runs   (  510.26 ms per token,     1.96 tokens per second)\n",
      "llama_print_timings:       total time =  230538.87 ms /   672 tokens\n",
      " 15%|█▌        | 3/20 [12:58<1:10:40, 249.42s/it]Llama.generate: 71 prefix-match hit, remaining 604 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =      14.34 ms /    64 runs   (    0.22 ms per token,  4461.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =  206866.94 ms /   604 tokens (  342.49 ms per token,     2.92 tokens per second)\n",
      "llama_print_timings:        eval time =   51750.73 ms /    63 runs   (  821.44 ms per token,     1.22 tokens per second)\n",
      "llama_print_timings:       total time =  258850.46 ms /   667 tokens\n",
      " 20%|██        | 4/20 [17:17<1:07:30, 253.15s/it]Llama.generate: 71 prefix-match hit, remaining 374 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =      24.04 ms /   106 runs   (    0.23 ms per token,  4408.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =  158433.60 ms /   374 tokens (  423.62 ms per token,     2.36 tokens per second)\n",
      "llama_print_timings:        eval time =   85624.77 ms /   105 runs   (  815.47 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time =  244372.53 ms /   479 tokens\n",
      " 25%|██▌       | 5/20 [21:21<1:02:29, 250.00s/it]Llama.generate: 71 prefix-match hit, remaining 594 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =      24.25 ms /   108 runs   (    0.22 ms per token,  4454.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =  253542.46 ms /   594 tokens (  426.84 ms per token,     2.34 tokens per second)\n",
      "llama_print_timings:        eval time =   89456.29 ms /   107 runs   (  836.04 ms per token,     1.20 tokens per second)\n",
      "llama_print_timings:       total time =  343307.09 ms /   701 tokens\n",
      " 30%|███       | 6/20 [27:04<1:05:44, 281.73s/it]Llama.generate: 71 prefix-match hit, remaining 570 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =      15.76 ms /    70 runs   (    0.23 ms per token,  4441.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =  242292.44 ms /   570 tokens (  425.07 ms per token,     2.35 tokens per second)\n",
      "llama_print_timings:        eval time =   56693.50 ms /    69 runs   (  821.64 ms per token,     1.22 tokens per second)\n",
      "llama_print_timings:       total time =  299237.76 ms /   639 tokens\n",
      " 35%|███▌      | 7/20 [32:04<1:02:17, 287.47s/it]Llama.generate: 71 prefix-match hit, remaining 416 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =      16.05 ms /    73 runs   (    0.22 ms per token,  4547.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =  175117.19 ms /   416 tokens (  420.95 ms per token,     2.38 tokens per second)\n",
      "llama_print_timings:        eval time =   59011.92 ms /    72 runs   (  819.61 ms per token,     1.22 tokens per second)\n",
      "llama_print_timings:       total time =  234300.04 ms /   488 tokens\n",
      " 40%|████      | 8/20 [35:58<54:06, 270.55s/it]  Llama.generate: 71 prefix-match hit, remaining 409 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =      56.54 ms /   254 runs   (    0.22 ms per token,  4492.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =  167334.93 ms /   409 tokens (  409.13 ms per token,     2.44 tokens per second)\n",
      "llama_print_timings:        eval time =  204486.82 ms /   253 runs   (  808.25 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time =  372787.71 ms /   662 tokens\n",
      " 45%|████▌     | 9/20 [42:11<55:27, 302.52s/it]Llama.generate: 71 prefix-match hit, remaining 693 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =      51.99 ms /   232 runs   (    0.22 ms per token,  4462.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =  296074.89 ms /   693 tokens (  427.24 ms per token,     2.34 tokens per second)\n",
      "llama_print_timings:        eval time =  196099.86 ms /   231 runs   (  848.92 ms per token,     1.18 tokens per second)\n",
      "llama_print_timings:       total time =  492773.61 ms /   924 tokens\n",
      " 50%|█████     | 10/20 [50:24<1:00:12, 361.27s/it]Llama.generate: 71 prefix-match hit, remaining 512 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =      11.37 ms /    78 runs   (    0.15 ms per token,  6858.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =  147706.43 ms /   512 tokens (  288.49 ms per token,     3.47 tokens per second)\n",
      "llama_print_timings:        eval time =   39736.41 ms /    77 runs   (  516.06 ms per token,     1.94 tokens per second)\n",
      "llama_print_timings:       total time =  187583.51 ms /   589 tokens\n",
      " 55%|█████▌    | 11/20 [53:31<46:13, 308.12s/it]  Llama.generate: 71 prefix-match hit, remaining 607 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =      22.99 ms /   139 runs   (    0.17 ms per token,  6046.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =  183216.91 ms /   607 tokens (  301.84 ms per token,     3.31 tokens per second)\n",
      "llama_print_timings:        eval time =   83593.83 ms /   138 runs   (  605.75 ms per token,     1.65 tokens per second)\n",
      "llama_print_timings:       total time =  267065.95 ms /   745 tokens\n",
      " 60%|██████    | 12/20 [57:58<39:25, 295.64s/it]Llama.generate: 71 prefix-match hit, remaining 626 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =       9.55 ms /    77 runs   (    0.12 ms per token,  8062.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =  373916.04 ms /   626 tokens (  597.31 ms per token,     1.67 tokens per second)\n",
      "llama_print_timings:        eval time =   35163.25 ms /    76 runs   (  462.67 ms per token,     2.16 tokens per second)\n",
      "llama_print_timings:       total time =  409154.58 ms /   702 tokens\n",
      " 65%|██████▌   | 13/20 [1:04:48<38:30, 330.03s/it]Llama.generate: 71 prefix-match hit, remaining 642 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =      12.68 ms /    76 runs   (    0.17 ms per token,  5991.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =  225015.95 ms /   642 tokens (  350.49 ms per token,     2.85 tokens per second)\n",
      "llama_print_timings:        eval time =   42067.66 ms /    75 runs   (  560.90 ms per token,     1.78 tokens per second)\n",
      "llama_print_timings:       total time =  267169.52 ms /   717 tokens\n",
      " 70%|███████   | 14/20 [1:09:15<31:06, 311.05s/it]Llama.generate: 71 prefix-match hit, remaining 612 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =      14.07 ms /    82 runs   (    0.17 ms per token,  5828.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =  220502.47 ms /   612 tokens (  360.30 ms per token,     2.78 tokens per second)\n",
      "llama_print_timings:        eval time =   45915.00 ms /    81 runs   (  566.85 ms per token,     1.76 tokens per second)\n",
      "llama_print_timings:       total time =  266521.56 ms /   693 tokens\n",
      " 75%|███████▌  | 15/20 [1:13:41<24:48, 297.63s/it]Llama.generate: 71 prefix-match hit, remaining 336 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =       8.80 ms /    84 runs   (    0.10 ms per token,  9541.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   77042.21 ms /   336 tokens (  229.29 ms per token,     4.36 tokens per second)\n",
      "llama_print_timings:        eval time =   32657.23 ms /    83 runs   (  393.46 ms per token,     2.54 tokens per second)\n",
      "llama_print_timings:       total time =  109869.75 ms /   419 tokens\n",
      " 80%|████████  | 16/20 [1:15:31<16:04, 241.12s/it]Llama.generate: 71 prefix-match hit, remaining 627 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =       8.35 ms /    65 runs   (    0.13 ms per token,  7783.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =  160708.24 ms /   627 tokens (  256.31 ms per token,     3.90 tokens per second)\n",
      "llama_print_timings:        eval time =   30555.66 ms /    64 runs   (  477.43 ms per token,     2.09 tokens per second)\n",
      "llama_print_timings:       total time =  191335.76 ms /   691 tokens\n",
      " 85%|████████▌ | 17/20 [1:18:43<11:18, 226.16s/it]Llama.generate: 71 prefix-match hit, remaining 646 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =      14.25 ms /    86 runs   (    0.17 ms per token,  6035.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =  208856.47 ms /   646 tokens (  323.31 ms per token,     3.09 tokens per second)\n",
      "llama_print_timings:        eval time =   48729.08 ms /    85 runs   (  573.28 ms per token,     1.74 tokens per second)\n",
      "llama_print_timings:       total time =  257719.12 ms /   731 tokens\n",
      " 90%|█████████ | 18/20 [1:23:00<07:51, 235.64s/it]Llama.generate: 71 prefix-match hit, remaining 347 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =      10.56 ms /    63 runs   (    0.17 ms per token,  5967.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =  135690.12 ms /   347 tokens (  391.04 ms per token,     2.56 tokens per second)\n",
      "llama_print_timings:        eval time =   37331.67 ms /    62 runs   (  602.12 ms per token,     1.66 tokens per second)\n",
      "llama_print_timings:       total time =  173175.33 ms /   409 tokens\n",
      " 95%|█████████▌| 19/20 [1:25:53<03:36, 216.89s/it]Llama.generate: 71 prefix-match hit, remaining 608 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =  169892.16 ms\n",
      "llama_print_timings:      sample time =      10.88 ms /    65 runs   (    0.17 ms per token,  5972.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =  222301.33 ms /   608 tokens (  365.63 ms per token,     2.74 tokens per second)\n",
      "llama_print_timings:        eval time =   37316.81 ms /    64 runs   (  583.08 ms per token,     1.72 tokens per second)\n",
      "llama_print_timings:       total time =  259754.89 ms /   672 tokens\n",
      "100%|██████████| 20/20 [1:30:13<00:00, 270.69s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "qa_dataset = generate_question_context_pairs(\n",
    "    nodes, llm=llm, num_questions_per_chunk=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the title of Virginie Despentes' book that is sometimes taught in gender studies and recommended to millennial women?\n"
     ]
    }
   ],
   "source": [
    "queries = qa_dataset.queries.values()\n",
    "print(list(queries)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [optional] save\n",
    "qa_dataset.save_json(\"pg_eval_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset = EmbeddingQAFinetuneDataset.from_json(\"pg_eval_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>queries</td>\n",
       "      <td>{'07161afe-0cc4-49bb-ae7e-c3702c268733': 'In w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>corpus</td>\n",
       "      <td>{'node_0': 'Virginie Despentes\n",
       "Despentes in Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relevant_docs</td>\n",
       "      <td>{'07161afe-0cc4-49bb-ae7e-c3702c268733': ['nod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mode</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0                                                  1\n",
       "0        queries  {'07161afe-0cc4-49bb-ae7e-c3702c268733': 'In w...\n",
       "1         corpus  {'node_0': 'Virginie Despentes\n",
       "Despentes in Ma...\n",
       "2  relevant_docs  {'07161afe-0cc4-49bb-ae7e-c3702c268733': ['nod...\n",
       "3           mode                                               text"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "eval_pdf= pd.DataFrame(qa_dataset)\n",
    "eval_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "metrics = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]\n",
    "\n",
    "if include_cohere_rerank:\n",
    "    metrics.append(\n",
    "        \"cohere_rerank_relevancy\"  # requires COHERE_API_KEY environment variable to be set\n",
    "    )\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    metrics, retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: In which year was Virginie Despentes born, and in which city in France?\n",
      "Metrics: {'hit_rate': 0.0, 'mrr': 0.0, 'precision': 0.0, 'recall': 0.0, 'ap': 0.0, 'ndcg': 0.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try it out on a sample query\n",
    "sample_id, sample_query = list(qa_dataset.queries.items())[0]\n",
    "sample_expected = qa_dataset.relevant_docs[sample_id]\n",
    "\n",
    "eval_result = retriever_evaluator.evaluate(sample_query, sample_expected)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Similarity Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import SemanticSimilarityEvaluator\n",
    "\n",
    "evaluator = SemanticSimilarityEvaluator(\n",
    "    embed_model=embed_model,\n",
    "    \n",
    "    similarity_threshold=0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \"DMA.\"\n",
    "reference = \"DMA.\"\n",
    "\n",
    "result = await evaluator.aevaluate(\n",
    "    response=response,\n",
    "    reference=reference,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.9999999999999998\n",
      "Passing:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Score: \", result.score)\n",
    "print(\"Passing: \", result.passing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
