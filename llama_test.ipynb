{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3ac9adb4",
      "metadata": {
        "id": "3ac9adb4"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/llm/llama_2_llama_cpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d5250ab7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement llama_cpp_python-0.2.88-cp311-cp311-win_amd64 (from versions: none)\n",
            "ERROR: No matching distribution found for llama_cpp_python-0.2.88-cp311-cp311-win_amd64\n"
          ]
        }
      ],
      "source": [
        "%pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host=files.pythonhosted.org --no-cache-dir  llama_cpp_python-0.2.88-cp311-cp311-win_amd64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aff273be",
      "metadata": {
        "id": "aff273be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\mlassoued\\appdata\\roaming\\python\\python311\\site-packages (4.42.4)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
            "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
            "     ----------------- -------------------- 20.5/43.7 kB 330.3 kB/s eta 0:00:01\n",
            "     -------------------------------------- 43.7/43.7 kB 711.0 kB/s eta 0:00:00\n",
            "Requirement already satisfied: filelock in c:\\users\\mlassoued\\.conda\\envs\\hey\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\mlassoued\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\mlassoued\\.conda\\envs\\hey\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\mlassoued\\.conda\\envs\\hey\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mlassoued\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mlassoued\\.conda\\envs\\hey\\lib\\site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in c:\\users\\mlassoued\\.conda\\envs\\hey\\lib\\site-packages (from transformers) (2.32.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mlassoued\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\mlassoued\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mlassoued\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mlassoued\\.conda\\envs\\hey\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mlassoued\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\mlassoued\\.conda\\envs\\hey\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mlassoued\\.conda\\envs\\hey\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mlassoued\\.conda\\envs\\hey\\lib\\site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mlassoued\\.conda\\envs\\hey\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mlassoued\\.conda\\envs\\hey\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
            "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
            "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.1/9.5 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.2/9.5 MB 2.1 MB/s eta 0:00:05\n",
            "   - -------------------------------------- 0.3/9.5 MB 2.6 MB/s eta 0:00:04\n",
            "   -- ------------------------------------- 0.5/9.5 MB 2.8 MB/s eta 0:00:04\n",
            "   -- ------------------------------------- 0.6/9.5 MB 2.9 MB/s eta 0:00:04\n",
            "   --- ------------------------------------ 0.8/9.5 MB 2.9 MB/s eta 0:00:04\n",
            "   --- ------------------------------------ 0.9/9.5 MB 3.0 MB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 1.0/9.5 MB 2.9 MB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 1.0/9.5 MB 2.6 MB/s eta 0:00:04\n",
            "   ---- ----------------------------------- 1.2/9.5 MB 2.7 MB/s eta 0:00:04\n",
            "   ----- ---------------------------------- 1.3/9.5 MB 2.6 MB/s eta 0:00:04\n",
            "   ------ --------------------------------- 1.4/9.5 MB 2.6 MB/s eta 0:00:04\n",
            "   ------ --------------------------------- 1.5/9.5 MB 2.6 MB/s eta 0:00:04\n",
            "   ------ --------------------------------- 1.6/9.5 MB 2.6 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 1.7/9.5 MB 2.6 MB/s eta 0:00:03\n",
            "   ------- -------------------------------- 1.8/9.5 MB 2.5 MB/s eta 0:00:04\n",
            "   -------- ------------------------------- 1.9/9.5 MB 2.5 MB/s eta 0:00:04\n",
            "   -------- ------------------------------- 2.1/9.5 MB 2.5 MB/s eta 0:00:03\n",
            "   --------- ------------------------------ 2.2/9.5 MB 2.5 MB/s eta 0:00:03\n",
            "   --------- ------------------------------ 2.3/9.5 MB 2.5 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 2.4/9.5 MB 2.5 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 2.5/9.5 MB 2.5 MB/s eta 0:00:03\n",
            "   ----------- ---------------------------- 2.6/9.5 MB 2.4 MB/s eta 0:00:03\n",
            "   ----------- ---------------------------- 2.7/9.5 MB 2.5 MB/s eta 0:00:03\n",
            "   ------------ --------------------------- 2.8/9.5 MB 2.5 MB/s eta 0:00:03\n",
            "   ------------ --------------------------- 2.9/9.5 MB 2.5 MB/s eta 0:00:03\n",
            "   ------------- -------------------------- 3.1/9.5 MB 2.5 MB/s eta 0:00:03\n",
            "   ------------- -------------------------- 3.2/9.5 MB 2.5 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 3.3/9.5 MB 2.5 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 3.5/9.5 MB 2.5 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 3.6/9.5 MB 2.5 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 3.7/9.5 MB 2.5 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 3.9/9.5 MB 2.5 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 4.0/9.5 MB 2.5 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 4.1/9.5 MB 2.6 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 4.3/9.5 MB 2.6 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 4.4/9.5 MB 2.6 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 4.5/9.5 MB 2.6 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 4.7/9.5 MB 2.6 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 4.8/9.5 MB 2.6 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 4.9/9.5 MB 2.6 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 5.0/9.5 MB 2.6 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 5.2/9.5 MB 2.6 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 5.3/9.5 MB 2.6 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 5.5/9.5 MB 2.6 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 5.6/9.5 MB 2.6 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 5.7/9.5 MB 2.6 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 5.9/9.5 MB 2.6 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 6.0/9.5 MB 2.6 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 6.1/9.5 MB 2.6 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 6.3/9.5 MB 2.7 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 6.5/9.5 MB 2.7 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 6.6/9.5 MB 2.7 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 6.7/9.5 MB 2.7 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 6.8/9.5 MB 2.7 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 7.0/9.5 MB 2.7 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 7.2/9.5 MB 2.7 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 7.3/9.5 MB 2.7 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 7.5/9.5 MB 2.7 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 7.6/9.5 MB 2.7 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 7.7/9.5 MB 2.7 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 7.9/9.5 MB 2.7 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 8.1/9.5 MB 2.8 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 8.2/9.5 MB 2.8 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 8.4/9.5 MB 2.8 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 8.5/9.5 MB 2.8 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 8.6/9.5 MB 2.8 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 8.8/9.5 MB 2.8 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 9.0/9.5 MB 2.8 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 9.1/9.5 MB 2.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------  9.3/9.5 MB 2.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 9.5/9.5 MB 2.8 MB/s eta 0:00:00\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.42.4\n",
            "    Uninstalling transformers-4.42.4:\n",
            "      Successfully uninstalled transformers-4.42.4\n",
            "Successfully installed transformers-4.44.2\n"
          ]
        }
      ],
      "source": [
        "%pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host=files.pythonhosted.org --no-cache-dir  openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "946a0597",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host=files.pythonhosted.org --no-cache-dir llama-index-llms-llama-cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf6c5b2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host=files.pythonhosted.org --no-cache-dir llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "40a33749",
      "metadata": {
        "id": "40a33749"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, load_index_from_storage, StorageContext\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.llms.llama_cpp.llama_utils import (\n",
        "    messages_to_prompt,\n",
        "    completion_to_prompt,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6fa0ec4f-03ff-4e28-957f-b4b99a0faa20",
      "metadata": {
        "id": "6fa0ec4f-03ff-4e28-957f-b4b99a0faa20",
        "outputId": "290ef844-73f3-4588-9d7b-21d5453c1cc5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 18 key-value pairs and 356 tensors from C:\\Users\\MLASSOUED\\Downloads\\obsidian-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = stablelm\n",
            "llama_model_loader: - kv   1:                               general.name str              = capy\n",
            "llama_model_loader: - kv   2:                    stablelm.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                  stablelm.embedding_length u32              = 2560\n",
            "llama_model_loader: - kv   4:                       stablelm.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:               stablelm.feed_forward_length u32              = 6912\n",
            "llama_model_loader: - kv   6:              stablelm.rope.dimension_count u32              = 20\n",
            "llama_model_loader: - kv   7:              stablelm.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:             stablelm.use_parallel_residual bool             = true\n",
            "llama_model_loader: - kv   9:      stablelm.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,50304]   = [\"<|endoftext|>\", \"<|padding|>\", \"!\",...\n",
            "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,50009]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"h e\", \"i n...\n",
            "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 0\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 50278\n",
            "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - type  f32:  130 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
            "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
            "llm_load_vocab: ************************************        \n",
            "llm_load_vocab:                                             \n",
            "llm_load_vocab: special tokens cache size = 52\n",
            "llm_load_vocab: token to piece cache size = 0.2987 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = stablelm\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 50304\n",
            "llm_load_print_meta: n_merges         = 50009\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 2560\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_rot            = 20\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 80\n",
            "llm_load_print_meta: n_embd_head_v    = 80\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 2560\n",
            "llm_load_print_meta: n_embd_v_gqa     = 2560\n",
            "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
            "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 6912\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = F16 (guessed)\n",
            "llm_load_print_meta: model params     = 2.80 B\n",
            "llm_load_print_meta: model size       = 5.21 GiB (16.00 BPW) \n",
            "llm_load_print_meta: general.name     = capy\n",
            "llm_load_print_meta: BOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 50278 '<|im_end|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: PAD token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: max token length = 1024\n",
            "llm_load_tensors: ggml ctx size =    0.16 MiB\n",
            "llm_load_tensors:        CPU buffer size =  5332.52 MiB\n",
            ".............................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 3904\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  1220.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1220.00 MiB, K (f16):  610.00 MiB, V (f16):  610.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   271.63 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1095\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'general.name': 'capy', 'stablelm.attention.layer_norm_epsilon': '0.000010', 'general.architecture': 'stablelm', 'stablelm.context_length': '4096', 'stablelm.rope.dimension_count': '20', 'stablelm.embedding_length': '2560', 'stablelm.block_count': '32', 'stablelm.feed_forward_length': '6912', 'stablelm.attention.head_count': '32', 'tokenizer.ggml.model': 'gpt2', 'stablelm.use_parallel_residual': 'true', 'tokenizer.ggml.bos_token_id': '0', 'tokenizer.ggml.eos_token_id': '50278', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ],
      "source": [
        "llm = LlamaCPP(\n",
        "    # You can pass in the URL to a GGML model to download it automatically\n",
        "    #model_url=model_url,\n",
        "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
        "    model_path= r\"C:\\Users\\MLASSOUED\\Downloads\\obsidian-f16.gguf\",\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
        "    context_window=3900,\n",
        "    # kwargs to pass to __call__()\n",
        "    generate_kwargs={},\n",
        "    # kwargs to pass to __init__()\n",
        "    # set to at least 1 to use GPU\n",
        "    model_kwargs={\"n_gpu_layers\": 0},\n",
        "    # transform inputs into Llama2 format\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cd0bbeb1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import configure_http_backend\n",
        "import os\n",
        "import urllib3\n",
        "import requests\n",
        " \n",
        "def backend_factory() -> requests.Session:\n",
        "    session = requests.Session()\n",
        "    session.verify = False\n",
        "    return session\n",
        " \n",
        "configure_http_backend(backend_factory=backend_factory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2242fd48",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d4c6f564",
      "metadata": {
        "id": "d4c6f564"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5d485f1e",
      "metadata": {
        "id": "5d485f1e"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\n",
        "    \"C:\\\\Users\\\\MLASSOUED\\Documents\\\\llam2\\\\data2\"\n",
        "\n",
        ").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e1f4f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c55c33cd",
      "metadata": {
        "id": "c55c33cd"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "9bada810",
      "metadata": {},
      "outputs": [],
      "source": [
        "index.storage_context.persist(persist_dir='./storage')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "980e3832",
      "metadata": {},
      "outputs": [],
      "source": [
        "index.set_index_id(\"vector_index\")\n",
        "index.storage_context.persist(\"./storage\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "c3529fba",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "c:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "ff688d7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
        "index = load_index_from_storage(storage_context, index_id=\"vector_index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e07659c8",
      "metadata": {
        "id": "e07659c8"
      },
      "outputs": [],
      "source": [
        "\n",
        "query_engine = index.as_query_engine(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "6671e8e3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =  100013.14 ms\n",
            "llama_print_timings:      sample time =      47.42 ms /   256 runs   (    0.19 ms per token,  5399.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =  177380.82 ms /   870 tokens (  203.89 ms per token,     4.90 tokens per second)\n",
            "llama_print_timings:        eval time =  153552.95 ms /   255 runs   (  602.17 ms per token,     1.66 tokens per second)\n",
            "llama_print_timings:       total time =  334554.00 ms /  1125 tokens\n"
          ]
        }
      ],
      "source": [
        "import tkinter as tk\n",
        "from tkinter import messagebox\n",
        "\n",
        "query_engine = index.as_query_engine(llm=llm)\n",
        "\n",
        "def get_response():\n",
        "    \n",
        "    wait_message.config(text=\"Processing the request...\")\n",
        "\n",
        "    query_text = query_entry.get()\n",
        "    response = query_engine.query(query_text).response\n",
        "\n",
        "    response_text.config(state=tk.NORMAL)  # Enable editing\n",
        "    response_text.delete(1.0, tk.END)  # Clear previous response\n",
        "    response_text.insert(tk.END, response)\n",
        "    response_text.config(state=tk.DISABLED)  # Make response text read-only\n",
        "\n",
        "    wait_message.config(text=\"Here's the response.\")\n",
        "\n",
        "root = tk.Tk()\n",
        "root.title(\"Chatbot Interface\")\n",
        "\n",
        "title_label = tk.Label(root, text=\"Chatbot\", font=(\"Helvetica\", 16, \"bold\"))\n",
        "query_entry = tk.Entry(root, width=70)\n",
        "response_text = tk.Text(root, width=80, height=10, wrap=tk.WORD)\n",
        "response_text.config(state=tk.DISABLED)  \n",
        "get_response_button = tk.Button(root, text=\"Get Response\", command=get_response, bg=\"#007acc\", fg=\"white\", font=(\"Helvetica\", 12))\n",
        "wait_message = tk.Label(root, text=\"\", font=(\"Helvetica\", 12))\n",
        "\n",
        "title_label.grid(row=0, column=0, columnspan=2, padx=10, pady=10)\n",
        "query_entry.grid(row=1, column=0, padx=10, pady=5)\n",
        "response_text.grid(row=2, column=0, padx=10, pady=5)\n",
        "get_response_button.grid(row=1, column=1, padx=10, pady=5)\n",
        "wait_message.grid(row=2, column=1, padx=10, pady=5)\n",
        "\n",
        "root.mainloop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f07ce4c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =  141919.99 ms\n",
            "llama_print_timings:      sample time =      20.21 ms /   256 runs   (    0.08 ms per token, 12665.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =  308634.70 ms /  1063 tokens (  290.34 ms per token,     3.44 tokens per second)\n",
            "llama_print_timings:        eval time =  122456.88 ms /   255 runs   (  480.22 ms per token,     2.08 tokens per second)\n",
            "llama_print_timings:       total time =  432127.94 ms /  1318 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "response\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query('résume le document fourni').response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e06bab31",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
        "r_=query_engine.query('résume le document fourni')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2abcc6e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8e9ca2b8",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/4 [00:00<?, ?it/s]Llama.generate: 512 prefix-match hit, remaining 105 prompt tokens to eval\n",
            "  0%|          | 0/4 [00:22<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m node_parser \u001b[38;5;241m=\u001b[39m SimpleNodeParser\u001b[38;5;241m.\u001b[39mfrom_defaults(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m      4\u001b[0m nodes \u001b[38;5;241m=\u001b[39m node_parser\u001b[38;5;241m.\u001b[39mget_nodes_from_documents(documents)\n\u001b[1;32m----> 5\u001b[0m qa_dataset \u001b[38;5;241m=\u001b[39m generate_question_context_pairs(\n\u001b[0;32m      6\u001b[0m     nodes,\n\u001b[0;32m      7\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m      8\u001b[0m     num_questions_per_chunk\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llama_index\\core\\llama_dataset\\legacy\\embedding.py:87\u001b[0m, in \u001b[0;36mgenerate_qa_embedding_pairs\u001b[1;34m(nodes, llm, qa_generate_prompt_tmpl, num_questions_per_chunk)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node_id, text \u001b[38;5;129;01min\u001b[39;00m tqdm(node_dict\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[0;32m     84\u001b[0m     query \u001b[38;5;241m=\u001b[39m qa_generate_prompt_tmpl\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     85\u001b[0m         context_str\u001b[38;5;241m=\u001b[39mtext, num_questions_per_chunk\u001b[38;5;241m=\u001b[39mnum_questions_per_chunk\n\u001b[0;32m     86\u001b[0m     )\n\u001b[1;32m---> 87\u001b[0m     response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mcomplete(query)\n\u001b[0;32m     89\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(response)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m     questions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     91\u001b[0m         re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, question)\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m result\n\u001b[0;32m     92\u001b[0m     ]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:235\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    232\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    233\u001b[0m )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 235\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llama_index\\core\\llms\\callbacks.py:429\u001b[0m, in \u001b[0;36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    420\u001b[0m event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[0;32m    421\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[0;32m    422\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    426\u001b[0m     },\n\u001b[0;32m    427\u001b[0m )\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 429\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m f(_self, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    431\u001b[0m     callback_manager\u001b[38;5;241m.\u001b[39mon_event_end(\n\u001b[0;32m    432\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[0;32m    433\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mEXCEPTION: e},\n\u001b[0;32m    434\u001b[0m         event_id\u001b[38;5;241m=\u001b[39mevent_id,\n\u001b[0;32m    435\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llama_index\\llms\\llama_cpp\\base.py:276\u001b[0m, in \u001b[0;36mLlamaCPP.complete\u001b[1;34m(self, prompt, formatted, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m formatted:\n\u001b[0;32m    274\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_to_prompt(prompt)\n\u001b[1;32m--> 276\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model(prompt\u001b[38;5;241m=\u001b[39mprompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_kwargs)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletionResponse(text\u001b[38;5;241m=\u001b[39mresponse[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], raw\u001b[38;5;241m=\u001b[39mresponse)\n",
            "File \u001b[1;32mc:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\llama_cpp\\llama.py:1791\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m   1728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1729\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1753\u001b[0m     logit_bias: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1754\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[0;32m   1755\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[0;32m   1756\u001b[0m \n\u001b[0;32m   1757\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_completion(\n\u001b[0;32m   1792\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m   1793\u001b[0m         suffix\u001b[38;5;241m=\u001b[39msuffix,\n\u001b[0;32m   1794\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[0;32m   1795\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   1796\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   1797\u001b[0m         min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[0;32m   1798\u001b[0m         typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[0;32m   1799\u001b[0m         logprobs\u001b[38;5;241m=\u001b[39mlogprobs,\n\u001b[0;32m   1800\u001b[0m         echo\u001b[38;5;241m=\u001b[39mecho,\n\u001b[0;32m   1801\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m   1802\u001b[0m         frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m   1803\u001b[0m         presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m   1804\u001b[0m         repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m   1805\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1806\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1807\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m   1808\u001b[0m         tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m   1809\u001b[0m         mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m   1810\u001b[0m         mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m   1811\u001b[0m         mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m   1812\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1813\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1814\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1815\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1816\u001b[0m         logit_bias\u001b[38;5;241m=\u001b[39mlogit_bias,\n\u001b[0;32m   1817\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\llama_cpp\\llama.py:1724\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1722\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[0;32m   1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[1;32m-> 1724\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(completion_or_chunks)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
            "File \u001b[1;32mc:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\llama_cpp\\llama.py:1208\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1206\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1207\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1208\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1209\u001b[0m     prompt_tokens,\n\u001b[0;32m   1210\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1211\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   1212\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[0;32m   1213\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[0;32m   1214\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   1215\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m   1216\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m   1217\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m   1218\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m   1219\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m   1220\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m   1221\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m   1222\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1223\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1224\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1225\u001b[0m ):\n\u001b[0;32m   1226\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llama_cpp\u001b[38;5;241m.\u001b[39mllama_token_is_eog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mmodel, token):\n",
            "File \u001b[1;32mc:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\llama_cpp\\llama.py:800\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(tokens)\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[0;32m    802\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m    803\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m    804\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    818\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[0;32m    819\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\llama_cpp\\llama.py:633\u001b[0m, in \u001b[0;36mLlama.eval\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    629\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[0;32m    631\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[0;32m    632\u001b[0m )\n\u001b[1;32m--> 633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
            "File \u001b[1;32mc:\\Users\\MLASSOUED\\.conda\\envs\\hey\\Lib\\site-packages\\llama_cpp\\_internals.py:357\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 357\u001b[0m return_code \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_decode(\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx,\n\u001b[0;32m    359\u001b[0m     batch\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[0;32m    360\u001b[0m )\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from llama_index.core.evaluation import generate_question_context_pairs\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
        "nodes = node_parser.get_nodes_from_documents(documents)\n",
        "qa_dataset = generate_question_context_pairs(\n",
        "    nodes,\n",
        "    llm=llm,\n",
        "    num_questions_per_chunk=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bcff97ae",
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "\n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llama_index\\core\\llms\\utils.py:42\u001b[0m, in \u001b[0;36mresolve_llm\u001b[1;34m(llm, callback_manager)\u001b[0m\n\u001b[0;32m     41\u001b[0m     llm \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[1;32m---> 42\u001b[0m     validate_openai_api_key(llm\u001b[38;5;241m.\u001b[39mapi_key)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llama_index\\llms\\openai\\utils.py:409\u001b[0m, in \u001b[0;36mvalidate_openai_api_key\u001b[1;34m(api_key)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
            "\u001b[1;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FaithfulnessEvaluator\n\u001b[1;32m----> 4\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m FaithfulnessEvaluator()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llama_index\\core\\evaluation\\faithfulness.py:129\u001b[0m, in \u001b[0;36mFaithfulnessEvaluator.__init__\u001b[1;34m(self, llm, raise_error, eval_template, refine_template, service_context)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    121\u001b[0m     llm: Optional[LLM] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m     service_context: Optional[ServiceContext] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Init params.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m llm_from_settings_or_context(Settings, service_context)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error \u001b[38;5;241m=\u001b[39m raise_error\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_template: BasePromptTemplate\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llama_index\\core\\settings.py:264\u001b[0m, in \u001b[0;36mllm_from_settings_or_context\u001b[1;34m(settings, context)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\u001b[38;5;241m.\u001b[39mllm\n\u001b[1;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mllm\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llama_index\\core\\settings.py:39\u001b[0m, in \u001b[0;36m_Settings.llm\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the LLM.\"\"\"\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m resolve_llm(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llama_index\\core\\llms\\utils.py:49\u001b[0m, in \u001b[0;36mresolve_llm\u001b[1;34m(llm, callback_manager)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`llama-index-llms-openai` package not found, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease run `pip install llama-index-llms-openai`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m         )\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 49\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     50\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load OpenAI model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTo disable the LLM entirely, set llm=None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m         )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     60\u001b[0m     splits \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: \n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******"
          ]
        }
      ],
      "source": [
        "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
        "\n",
        "\n",
        "evaluator = FaithfulnessEvaluator()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ea86e71",
      "metadata": {},
      "source": [
        "Tell me a question i can ask about the document."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
